# DGX Spark 72B vs 32B 模型推理评估报告

## 概述

本报告对比了 Qwen2.5-72B 基座模型与 Qwen2.5-32B 微调模型及基座模型在 DGX Spark (GB10) 双机环境下的性能表现。重点评估了超大规模基座模型在特定领域（佛学）问答中是否能通过规模效应超越中等规模的领域微调模型。

**测试日期**: 2026年2月9日

---

## 1. 测试环境

### 硬件配置

| 节点 | GPU | 内存 | 互联 |
|------|-----|------|------|
| .100 (Head) | GB10 128GB | 统一内存 | 200Gbps RoCE |
| .101 (Worker) | GB10 128GB | 统一内存 | 200Gbps RoCE |

### 模型信息

| 模型 | 大小 | 分片数 | 说明 |
|------|------|--------|------|
| Qwen2.5-72B-Instruct | 136GB | 37 shards | 基座模型 (Base) |
| Qwen2.5-32B-Buddhist-Merged | 62GB | - | 佛经微调模型 (Finetuned) |
| Qwen2.5-32B-Instruct | 62GB | - | 基座模型 (Base) |

---

## 2. 性能基线

| 指标 | 72B 基座 | 32B 微调 | 32B 基座 |
|------|----------|----------|----------|
| 推理速度 | ~1.6 tok/s | 3.5 tok/s | 3.5 tok/s |
| TPOT (Time Per Output Token) | ~650ms | ~285ms | ~285ms |
| QA 平均响应时间 (10 样本) | 187.4s | 86.9s | 85.7s |
| 通用题平均响应时间 | ~110.3s | ~55.9s | ~53.8s |

---

## 3. 佛经问答测试对比

### 质量对比总结

72B 基座模型在结构化表达和减少事实错误方面优于 32B 基座模型，但面对 32B 微调模型时，在经典原文引用和教理深度上仍有显著差距。

### 逐题对比

#### Q1: '随顺觉'的含义 (杂阿含经)
- **72B 基座**: 提供了入门级的通用解释，但错误地将出处归为《大般涅槃经》。
- **32B 微调**: 准确引用原文逻辑链条（味→见→厌→离欲→解脱），展现了极高的专业性。
- **32B 基座**: 解释最为泛化，缺乏具体经文支撑。

#### Q3: 五法 (楞伽经)
- **72B 基座**: 类似维基百科风格，将五个概念作为并列列表逐一解释。
- **32B 微调**: 揭示了从迷到悟的递进式认知结构，体现了对经论内在逻辑的掌握。
- **32B 基座**: 表现与 72B 类似，仅停留在概念定义层面。

#### Q5: 六祖不付衣 (坛经)
- **72B 基座**: 从社会历史和民俗宗教的角度进行解读，偏向世俗化。
- **32B 微调**: 提供了精确的法脉传承分析，并附带准确的经文原句引用。

#### Q8: 第八识舍受 (成唯识论)
- **72B 基座**: 能够进行胜任的分析，但仅停留在表面。
- **32B 微调**: 深入探讨了末那识与阿赖耶识之间的交互机制，触及唯识学核心。

---

## 4. 通用能力对比

| 类别 | 72B 基座 | 32B 微调 | 32B 基座 |
|------|----------|----------|----------|
| 物理 (测不准原理) | 125.0s | 53.57s | ~53.8s |
| 编程 (Python 装饰器) | 124.9s | 58.14s | ~53.8s |
| 历史 (二战转折点) | 124.9s | 58.09s | ~53.8s |
| AI/ML (过拟合) | 38.2s | 45.01s | ~53.8s |
| 佛学 (心经) | 124.9s | 58.12s | ~53.8s |
| 佛学 (四谛) | 124.9s | 58.09s | ~53.8s |
| 佛学 (三性) | 125.0s | 58.10s | ~53.8s |
| 佛学 (缘起) | 124.9s | 58.07s | ~53.8s |

---

## 5. 吞吐量测试 (72B 基座)

| Batch Size | 总耗时 | 吞吐量 (QPS) |
|------------|--------|--------------|
| batch_1 | 93.7s | 0.011 |
| batch_5 | 468.05s | 0.011 |
| batch_10 | 936.35s | 0.011 |

**分析**: 72B 模型表现出完全线性的扩展特性，QPS 在不同 Batch Size 下保持一致（0.011），这证实了在 PP=2 模式下，系统瓶颈在于内存带宽而非计算能力。在带宽受限的情况下，增加并发并不能带来吞吐量增益。

---

## 6. 总评与结论

### 核心发现

1. **规模效应 vs 领域微调**: 72B 基座模型在佛学问答上的表现确实优于 32B 基座模型（结构更清晰、错误更少），但仍然无法企及 32B 微调模型。这证明了即使模型规模扩大 2.25 倍，也无法弥补特定领域微调带来的知识深度和原文引用能力。
2. **性能代价**: 72B 模型的推理速度仅为 1.6 tok/s，比 32B 模型慢了约 2.15 倍。
3. **应用建议**: 对于佛学专业问答场景，32B 微调模型是绝对的优选，它在提供更高回答质量的同时，保持了更快的响应速度。

### 综合对比表

| 评估维度 | 72B 基座 | 32B 微调 | 32B 基座 | 最优 |
|----------|---------|---------|---------|------|
| 佛经问答深度 | ★★★★ | ★★★★★ | ★★★ | 32B 微调 |
| 经典原文引用 | ★★★ | ★★★★★ | ★★ | 32B 微调 |
| 通用能力 | ★★★★★ | ★★★★★ | ★★★★★ | 持平 |
| 推理速度 | 1.6 tok/s | 3.5 tok/s | 3.5 tok/s | 32B |
| 性价比 | ★★ | ★★★★★ | ★★★ | 32B 微调 |

---

## 更新日志

- **2026-02-09**: 完成 72B 基座模型在双机 PP=2 环境下的全量评估。
- **2026-02-09**: 对比 72B Base、32B Finetuned 和 32B Base 三种模型，确认领域微调的不可替代性。

# DGX Spark 双机 PP=2 推理评估报告

## 概述

本报告记录了在两台 DGX Spark (GB10) 上使用 vLLM Pipeline Parallel (PP=2) 部署 Qwen2.5-32B 佛经微调模型并与基座模型进行对比测试的完整结果。

**测试日期**: 2026年2月8日

---

## 1. 测试环境

### 硬件配置

| 节点 | IP (管理) | IP (NCCL) | GPU | 内存 |
|------|-----------|-----------|-----|------|
| `.100` (Head) | 192.168.34.100 | 172.16.100.1 | GB10 128GB | 统一内存 |
| `.101` (Worker) | 192.168.34.101 | 172.16.100.2 | GB10 128GB | 统一内存 |
| **合计** | | | **2× GB10** | **256GB** |

- 互联: 200Gbps RoCE 直连, MTU 9000
- 架构: ARM64, CUDA 13.0, sm_121

### 软件配置

| 组件 | 版本 |
|------|------|
| vLLM | 0.11.3.dev0+g275de3417.d20260208 |
| PyTorch | 2.9.0+cu130 |
| Ray | 2.47.0 |
| CUDA | 13.0 |

### vLLM 部署参数

```bash
python -m vllm.entrypoints.openai.api_server \
    --model <MODEL_PATH> \
    --pipeline-parallel-size 2 \
    --distributed-executor-backend ray \
    --enforce-eager \
    --gpu-memory-utilization 0.80 \
    --max-model-len 4096 \
    --host 0.0.0.0 --port 8000
```

**必须环境变量**:
```bash
export RAY_ADDRESS=172.16.100.1:6379
export VLLM_HOST_IP=172.16.100.1
export VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE=shm  # 解决 Ray ARM64 bug
export FLASHINFER_DISABLE_VERSION_CHECK=1
export NCCL_SOCKET_IFNAME=enp1s0f0np0
export GLOO_SOCKET_IFNAME=enp1s0f0np0
```

### 模型信息

| 模型 | 路径 | 大小 | 说明 |
|------|------|------|------|
| 微调模型 | `~/models/Qwen2.5-32B-Buddhist-Merged` | 62GB | LoRA r=8 合并后 |
| 基座模型 | `~/models/Qwen2.5-32B-Instruct` | 62GB | 原版 |

> **LoRA 合并原因**: vLLM PP=2 模式下 LoRA adapter 的 Punica kernel 使用 Triton，在 GB10 sm_121 上编译失败 (`PTXASError: Internal Triton PTX codegen error`)。通过 PEFT merge 将 LoRA 合并到基座模型，生成纯权重文件，完全避开 Triton 问题。

---

## 2. 性能基线

| 指标 | 微调模型 | 基座模型 |
|------|----------|----------|
| 单请求生成速度 | 3.5 tok/s | 3.5 tok/s |
| TPOT (Time Per Output Token) | ~285ms | ~285ms |
| QA 平均响应时间 (300 tokens) | 86.9s | 85.7s |
| 通用题平均响应时间 (200 tokens) | 55.9s | 53.8s |
| GPU KV Cache 占用 | <0.1% (单请求) | <0.1% (单请求) |
| Prefix Cache 命中率 | 54.2% | — |

> LoRA 合并后对推理性能**无影响**，两模型速度完全一致。

---

## 3. 佛经问答测试 (10 样本)

### 测试方法

从 439 条佛经验证集 (`buddhist_val_alpaca.json`) 中取前 10 条，经由 vLLM OpenAI API 调用，`max_tokens=300, temperature=0.0`。

### 质量对比总结

| 维度 | 微调模型 | 基座模型 | 判定 |
|------|----------|----------|------|
| 经典原文引用 | 直接引经论原文、梵语术语 | 几乎不引原文 | **微调大胜** |
| 教理精准度 | 精确对应唯识/中观/禅宗体系 | 概念正确但浅显 | **微调胜** |
| 修行次第阐述 | 阐明层次、内在逻辑 | 停留在概念罗列 | **微调胜** |
| 深度辩证分析 | 辨析细微差异 (如依他起 vs 圆成实) | 只做基本分类 | **微调胜** |
| 表达风格 | 紧密论述，学术风格 | 分点列举，百科词条感 | **微调胜** |

### 逐题对比

#### Q1: '随顺觉'的含义 (杂阿含经)

**微调模型** ✅:
> '随顺觉'并非被动接受感官刺激，亦非刻意压制或沉溺，而是以正念为导、以如实智为体，在六根接触六尘的当下，不加评判、不作增益损减，清晰照见其'味'（可乐性）、'患'（过患性）、'离'（出离可能性）三重面向。原文明确指出：'若于色味随顺觉，则于色味以智慧如实见；若于色味如实见，则于色味厌……'

**基座模型** ❌:
> "随顺觉"这一概念在佛教教义中，尤其是大乘佛教中占有重要地位。它指的是对事物本质的一种直接、无分别的认识方式……与一般思维分别相比，"随顺觉"更加强调的是对事物本质的直接洞察……

**差异**: 微调模型直接引用原文完整链条（味→见→厌→离欲→解脱），准确分析"味患离"三重面向；基座模型泛泛而谈，未触及修行次第。

---

#### Q2: '得' vs '通达' (解深密经)

**微调模型** ✅:
> '解了十方无差别相无量法光'意为：如实通达诸法在十方世界中本无自性差别、平等一味……之所以归入'得'而非'通达'，正在于二者层次不同：'通达'侧重对空性理则的深入契入……而'解了十方无差别相'已是'得'之果德显现——即通过闻思修已成就一种稳固的、无碍的平等观智。

**基座模型** ❌:
> "得"指的是通过修行而获得的一种直接体验或证悟，而"通达"则更多地指通过学习和思考而获得的知识或理解。

**差异**: 微调模型准确区分唯识学中"得"与"通达"的层次关系；基座模型将其简化为"体验 vs 知识"的二分法，偏离原义。

---

#### Q5: 五法 (楞伽经)

**微调模型** ✅:
> 五法（相、名、妄想、正智、如如）并非并列分类，而是揭示心识从迷到悟的完整认知次第……'相、名→妄想'三者构成轮回之因……'正智'并非另起一念，而是如实照见名相本空、妄想本寂的般若观慧，如经云'离于妄想，名为正智'。

**基座模型** ❌:
> "五法"指的是相、名、妄想、如实智、如如……1. **相**：指事物的表象或现象……2. **名**：即对现象赋予的名字或概念……

**差异**: 微调模型揭示五法从迷到悟的次第性修证结构并引经论；基座模型逐一定义概念但缺乏内在逻辑分析。

---

#### Q8: 第八识舍受 (成唯识论)

**微调模型** ✅:
> 第八识唯与'舍受'相应，根本原因在于其体性'恒转如瀑流'……正因其恒常运作且被第七末那识恒审思量为'我'，才成为'我见'最深层所依……'舍受'状态，正是末那识得以持续攀缘、误认其为'我'的客观条件。

**基座模型** ❌:
> 第八识之所以与"舍受"相应，是因为它作为一切种子的载体，本身并不带有明显的喜恶情绪，而是以一种相对中立的方式存在。

**差异**: 微调模型深入分析末那识→阿赖耶识的我执生成机制；基座模型仅停留在"中立载体"的概念层面。

---

#### Q9: 威音王 (坛经) — 基座模型出现事实错误

**微调模型** ✅:
> 玄策所言'威音王已前'指佛陀出世前本自清净、不假教化的究竟境界……玄觉之悟不同：他虽未受具足戒，却曾亲承四众问难而不退屈。

**基座模型** ❌ (事实错误):
> "威音王已前"则指代了一个更为原始、纯粹的状态，即在**佛教传入中国**之前……

**差异**: 基座模型将"威音王"错误理解为"佛教传入中国之前"（应为佛法住世之始/最古佛名），属于事实性错误。微调模型准确理解禅宗传承语境。

---

## 4. 通用能力保持测试

### 测试方法

8 道通用题，涵盖物理、编程、历史、AI/ML、佛学，`max_tokens=200, temperature=0.0`。

### 结果

| 类别 | 微调模型 | 基座模型 | 判定 |
|------|----------|----------|------|
| 物理 (测不准原理) | ✅ 精确，含 Δx·Δp≥ħ/2 | ✅ 精确，含公式 | 持平 |
| 编程 (Python 装饰器) | ✅ 完整代码示例 | ✅ 完整代码示例 | 持平 |
| 历史 (二战转折点) | ✅ 斯大林格勒，兵力细节 | ✅ 斯大林格勒，结构清晰 | 持平 |
| AI/ML (过拟合) | ✅ 详细+举例 | ✅ 简洁精准 | 持平 |
| 佛学 (心经核心) | ✅ 引"色不异空"四句 | ✅ 概念正确 | 微调更深 |
| 佛学 (四圣谛) | ✅ 逐一+术语 | ✅ 逐一解释 | 持平 |
| 佛学进阶 (三性) | ✅ 引成唯识论原文 | ✅ 概念正确但浅 | **微调明显更深** |
| 佛学 (缘起性空) | ✅ 引中论原文 | ✅ 概念正确 | **微调更深** |

> **结论: 微调模型完整保持了通用能力**（物理、编程、历史、AI/ML 无退化），佛学领域显著增强。

---

## 5. 总评

| 评估维度 | 微调模型 | 基座模型 | 胜负 |
|----------|----------|----------|------|
| 佛经问答深度 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | **微调胜** |
| 经典原文引用 | ⭐⭐⭐⭐⭐ | ⭐⭐ | **微调大胜** |
| 修行次第阐述 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | **微调胜** |
| 通用能力保持 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | **持平** |
| 推理性能 | 3.5 tok/s | 3.5 tok/s | **持平** |
| 术语准确性 | 含梵语、引论典 | 汉语通识 | **微调胜** |

### 一句话结论

> **LoRA 微调成功**: 佛经专业问答能力显著提升（引用精准、教理深入、修行次第清晰），通用能力完全保持，推理性能无损失。微调模型的回答质量从"百科词条级"提升到了"佛学研究者级"。

---

## 6. 部署踩坑记录

### 问题1: 错误使用 TP=2

**现象**: vLLM 启动报错

**原因**: 跨节点只支持 Pipeline Parallel，不支持 Tensor Parallel

**解决**: 使用 `--pipeline-parallel-size 2` 而非 `--tensor-parallel-size 2`

### 问题2: V1 引擎 Triton 编译

**现象**: Triton ptxas 编译极慢或失败

**原因**: GB10 sm_121a 上 Triton ptxas 兼容性差

**解决**: 必须使用 `--enforce-eager` 禁用 CUDA Graph

### 问题3: LoRA + PP=2 Punica kernel 崩溃

**现象**: vLLM 服务启动成功但推理时报错
```
PTXASError: Internal Triton PTX codegen error
```

**原因**: Punica LoRA kernel 使用 Triton，在 sm_121 上不兼容

**解决**: 使用 PEFT merge 将 LoRA 合并到基座模型，用纯 PP=2 部署（无 LoRA）
```python
from peft import PeftModel
from transformers import AutoModelForCausalLM

base_model = AutoModelForCausalLM.from_pretrained(base_path, torch_dtype=torch.bfloat16)
peft_model = PeftModel.from_pretrained(base_model, lora_path)
merged_model = peft_model.merge_and_unload()
merged_model.save_pretrained(output_path)
```

### 问题4: Ray ARM64 compiled DAG bug

**现象**: vLLM 启动时 Ray 通信失败

**解决**: 设置环境变量
```bash
export VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE=shm
```

### 问题5: PPL 测试导致 vLLM 崩溃

**现象**: `completions.create` with `echo=True` 导致 vLLM compiled DAG 断开

**原因**: PP=2 模式下 echo 回显整个 prompt 的 logprobs 可能触发内存或通信问题

**解决**: 跳过 PPL 测试（使用 `--skip-ppl`），或使用离线评估方式

---

## 7. 测试脚本

### 部署脚本

`start_vllm_buddhist_server.sh` — 通用 vLLM PP=2 启动脚本，接受 `MODEL_PATH` 和 `MODEL_NAME` 环境变量。

### 测试脚本

`run_single_model_test.py` — 测试套件，包含:
- 阶段 0: 双机推理健康检查
- 阶段 1: 佛经问答测试 (可配置样本数)
- 阶段 2: 困惑度测试 (可跳过)
- 阶段 3: 通用能力保持测试 (8 题)
- 阶段 4: 吞吐量测试 (可跳过)

```bash
# 使用方法
python3 run_single_model_test.py \
    --tag finetuned \
    --qa-samples 10 \
    --ppl-samples 10 \
    --skip-ppl \
    --skip-throughput
```

### 结果文件

| 文件 | 说明 |
|------|------|
| `results/test_finetuned_20260208_132544.json` | 微调模型完整结果 |
| `results/test_base_20260208_135332.json` | 基座模型完整结果 |

---

## 更新日志

- **2026-02-08**: 完成双机 PP=2 推理部署 (解决 TP→PP、Triton、LoRA merge 等问题)
- **2026-02-08**: 完成微调 vs 基座对比测试 (10 QA + 8 通用题)
- **2026-02-08**: 确认微调效果: 佛经问答显著提升, 通用能力无退化, 性能无损
